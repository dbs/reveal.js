<!doctype html>
<html lang="en">

    <head>
        <meta charset="utf-8">

        <title>White hat SEO: Structured Web Data</title>

        <meta name="description" content="How to add structured data to your library web site">
        <meta name="author" content="Dan Scott">

        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

        <link rel="stylesheet" href="css/reveal.css">
        <link rel="stylesheet" href="css/theme/white.css" id="theme">

        <!-- Code syntax highlighting -->
        <link rel="stylesheet" href="lib/css/googlecode.css">
        <style>
.reveal h1, .reveal h2, .reveal h3, .reveal h4, .reveal h5, .reveal h6 {
    text-transform: none;
} 
.reveal pre code {
    background: inherit;
    color: inherit;
}
        </style>

        <!-- Printing and PDF exports -->
        <script>
            var link = document.createElement( 'link' );
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
            document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->
    </head>

    <body>

        <div class="reveal">
            <div class="slides">
                <section>
                    <h1>White hat SEO:</h1>
                    <h2>Structured Web Data</h2>
                    <p>
                        <small><a href="https://coffeecode.net">Dan Scott</a> / <a href="http://google.com/+DanScottCAN">+DanScottCAN</a></small>
                    </p>
                </section>

                <section>
                    <section>
                        <h2>Search engines vs. libraries</h2>
                        <ul>
                            <li><em>Goal</em>: Connect users with your resources using their search tool.
                                <ul>
                                    <li class="fragment">People are working on this problem!</li>
                                    <li class="fragment">Ask me about Evergreen, Koha, or VuFind!</li>
                                </ul>
                            </li>
                            <li class="fragment"><em>But first</em>: Connect users with your library using their search tool.</li>
                        </ul>
                        <aside class="notes">
                            <p>
                            It is still a matter of debate in some circles (AUTOCAT) as to whether
                            trying to reach users through general search engines like Google, Bing,
                            and Yahoo! is even a worthwhile goal. Information literacy classes,
                            the splendors of advanced search interfaces, etc.
                            </p>
                            <p>
                            A recurring topic in the library world is how we might get the
                            contents of our catalogues to show up in search engines so that regular users
                            could be pointed directly at our resources in the course of their general
                            searches. (images of twitter threads, mailing list threads, etc)
                            </p>
                            <p>
                            However, at a very basic level, if someone searches for "library" on their
                            phone or laptop, you want your library to show up if they happen to be in the
                            general vicinity.
                            </p>
                        </aside>
                    </section>

                    <section>
                        <h2>When you search for "library"...</h2>
                    </section>

                    <section data-background="images/library_search_bing.png">
                        <aside class="notes">
                            Toronto <em>is</em> the centre of the universe! But hey,
                            the Greater Sudbury Public Libraries home page is second, not bad.
                            Let's dig into the local results.
                        </aside>
                    </section>

                    <section data-background="images/library_search_bing_on_crack.png">
                        <aside class="notes">
                            By clicking on Local Results we can see just how local those results
                            are, matching URLs from Ottawa and BC to Sudbury libraries. Much of
                            the data seems to be driven by Foursquare data.
                        </aside>
                    </section>

                    <section data-background="images/library_search_foursquare.png">
                        <aside class="notes">
                            Foursquare is user-driven information; you might want to become a user
                            to try to sort things out about your library. Check the images for a
                            fun unfiltered impression of your library!
                        </aside>
                    </section>

                    <section data-background="images/library_search_yahoo_local_fail.png">
                        <aside class="notes">
                            Searching Yahoo! via Firefox results in a complete unwillingness
                            to acknowledge that Canada even exists.
                        </aside>
                    </section>

                    <section data-background="images/library_search_yahoo_local_fail_2.png">
                        <aside class="notes">
                            Searching Yahoo! via yahoo.ca leads me to their partner in crime,
                            YellowPages, which shows me all the libraries around
                            Sudbury, but not the ones actually <em>in</em> Sudbury.
                        </aside>
                    </section>

                    <section data-background="images/library_search_google.png">
                        <aside class="notes">
                            Ah, Google. Toronto is still the centre of the world, but now
                            we have a reasonably ordered set of local results with reasonably
                            accurate information. Let's dig into those local results.
                        </aside>
                    </section>

                
                    <section data-background="images/library_search_click_map.png">
                        <aside class="notes">
                            Clicking on one of the icons on the Google result map gives us
                            a quick overview of the library.
                        </aside>
                    </section>

                    <section data-background="images/library_search_click_map_gplus.png">
                        <aside class="notes">
                            Another click takes you to the library's Google+ page, which
                            may be driving the source of the data for search results.
                        </aside>
                    </section>

                    <section>
                        <h2>Getting your library into search</h2>
                        <ul>
                            <li>Populate social media profiles with branch addresses, hours, contact info, pictures...
                            <ul>
                                <li>Google: create Google+ pages</li>
                                <li>Bing: populate Foursquare</li>
                                <li>Yahoo!: maybe take out a Yellow Pages ad?</li>
                            </ul>
                            </li>
                            <li class="fragment">But that adds more places to maintain copies of data</li>
                        </ul>
                    </section>

                    <section>
                        <h2>OCLC to the rescue?</h2>
                        <ul>
                            <li>The <a href="https://www.oclc.org/worldcat-registry.en.html">WorldCat Registry claims</a>
                                <ul>
                                <li>to be the <q>Authoritative single source for institutional metadata</q></li>
                                <li>that <q>Built-in Web services distribute Registry data across the Web, enhancing Web discovery of your content and services</q></li>
                                </ul>
                            </li>
                            <li class="fragment">Alas, the claims differ wildly from reality.</li>
                        </ul>
                        <aside class="notes">
                            Despite these bold claims, even if a search engine ponied up for the commercial
                            license needed to use the WorldCat Registry data, it would be very dirty data at
                            best.
                        </aside>
                    </section>

                    <section>
                        <a href="http://www.worldcat.org/registry/Institutions/67970"><img src="images/library_search_worldcat_registry_gspl.png" alt="WorldCat Registry showing almost no data for Greater Sudbury Public Library"></a>
                        <aside class="notes">
                        GSPL shows up with almost no info--just a name and an address for the main
                        branch--and no branch information.
                        </aside>
                    </section>

                    <section>
                        <a href="http://www.worldcat.org/registry/Institutions/87164"><img src="images/library_search_worldcat_registry_jnd.png" alt="WorldCat Registry showing some data for J.N. Desmarais Library"></a>
                        <aside class="notes">
                            Laurentian shows up with more information--largely because I updated it in the
                            course of my research to point at the "new" catalogue we had migrated to 5
                            years ago--but still no branch info and a bit of a conflict between ILL needs
                            and general needs of the library.

                            But even relatively static data such as operating hours can be cumbersome to
                            maintain across multiple social media sites. There has to be a better way.
                        </aside>
                    </section>

                    <section>
                        <a href="http://www.worldcat.org/registry/Institutions/65081"><img src="images/library_search_worldcat_registry_tpl.png" alt="WorldCat Registry showing almost no data for Toronto Public Library"></a>
                        <aside class="notes">
                            Even TPL has very little information; two branches
                            are listed, no phone number or email address, and almost certainly the wrong
                            OpenURL resolver.
                        </aside>
                    </section>

                    <section>
                        <h2>Are you convinced we have a visibility challenge?</h2>
                    </section>

                    <section>
                        <h2>SEO is not a dirty word</h2>
                        <blockquote cite="http://www.bing.com/webmaster/help/webmaster-guidelines-30fba23a">
                            [SEO] is a valid practice
                            which seeks to improve technical and content aspects of a website, making the
                            content easier to find, relevant, and more accessible to the search engine
                            crawlers.
                        </blockquote>
                        <aside class="notes">
                            <p>
                            Search engine optimization (SEO) has acquired a dirty name, by association
                            with spammers who want to sell you snake oil that will help your web site rise
                            in the ranks of relevancy. Web rings, spammy comments on blogs, and all kinds
                            of base tricks have slandered the perfectly valid pursuit of trying to make the
                            fantastic content and services you have to offer discoverable to the people who
                            might benefit from it.
                            </p>

                            <p>
                            Just like the term hackers was once a noble term associated with those who
                            were interested in how technology works and how to make it work better, then
                            spoiled by association with script kiddies and trolls who just wanted to make
                            lives miserable, we started to reclaim the term by differentiating "white hat"
                            hackers vs. "black hat" hackers. So, too, are we reclaiming the term SEO by
                            differentiating "white hat" SEO vs. "black hat" SEO.
                            </p>

                            <p>
                            In this session, I'm going to walk through some of the basics that you need
                            to look at from a white hat SEO perspective. If you have internet connectivity,
                            I invite you to take a look at your own site as I work through some of the
                            examples I've already prepared; if not, I'll do my best to make the examples
                            come alive on the screen. If at any time you have a question, please fire away.
                            </p>
                        </aside>
                    </section>
                    <section>
                        <h2>Not just structured data</h2>
                        <p>Take care of the basics first!</p>
                        <ul>
                            <li>Cool, persistent URLs</li>
                            <li>Accessible HTML</li>
                            <li><code>robots.txt</code></li>
                            <li>Sitemaps</li>
                        </ul>
                        <aside class="notes">
                            I know I specifically mentioned "structured data", and I'm going to get there,
                            but having evaluated many different library websites since submitting the
                            initial proposal for this session, there are a few prerequisites that we need
                            to talk about first.
                        </aside>
                    </section>
                    <section>
                        <h2>Cool, persistent URLs</h2>
                        <ul>
                            <li><a href="http://www.w3.org/Provider/Style/URI.html">Cool URIs don't change</a></li>
                            <li>Links to your web content increases its ranking</li>
                            <li>Hide the publishing mechanisms and focus on the content!
                                <ul>
                                    <li><em>Bad:</em> http://example.ca/CMS-product/index.php?JSESSIONID=abc123;page=hours</li>
                                    <li><em>Good:</em> http://example.ca/hours</li>
                                </ul>
                            </li>
                        </ul>
                        <aside class="notes">
                            <p>
                            If your users can't get back to the same content using the same URL,
                            then neither will search engines.
                            </p>
                            <p>
                            Design for not just 6 months in the future, but six years.
                            </p>
                        </aside>
                    </section>
                    <section>
                        <h2>Accessible HTML</h2>
                        <ul>
                            <li>Cynthia Ng talks about <a href="https://cynng.wordpress.com/2015/01/22/making-web-services-accessible-with-universal-design/">accessibility as Universal Design</a>
                            <ul>
                                <li>Use meaningful HTML elements, <code>@class</code> attributes, and CSS.</li>
                                <li>Don't use an image where text would do.</li>
                                <li>ALT text, captions, and transcriptions help machines as well as humans.</li>
                            </ul>
                            </li>
                            <li>But you're all <a href="http://www.mcss.gov.on.ca/en/mcss/programs/accessibility/info_sheets/info_comm/website.aspx">AODA-compliant</a>, right?</li>
                        </ul>
                        <aside class="notes">
                            <p>
                            This is where we talk a bit about the difference between information that
                            humans can see and interpret, versus what machines can see and interpret. At a
                            very basic level, there is a lot of overlap here with principles of information
                            design and designing for accessibility: for example:
                            </p>
                            <p>
                            Use meaningful HTML elements, such as headings, rather than forcing everything
                            into tables or this decade's equivalent: div/span soup. And use those headings
                            hierarchically; the H1 should be the title of the page, then H2 should be used
                            to break out separate sections of the page into major topics; then H3 should
                            subdivide the content of those major topics. If you find yourself using a lot
                            of strong/bold tags or CSS styles, maybe you just need to carve up your content
                            a little bit more.
                            </p>
                            <p>
                            Don't use an image where text would do. For example, don't
                            create a PNG or JPEG to use as a button that says "Enter the contest" as an
                            image; even if you add alt="Enter the contest", that's less approachable than
                            simply using "Enter the contest" as the actual text of the button.
                            </p>
                            <p>
                            Again, accessible information design has been the subject of many other talks.
                            Given that we're in Ontario and AODA legislation is in effect, you're all at
                            least compliant with WCAG 2.0 level A, right? Good!
                        </aside>
                    </section>

                </section>

                <section>
                    <section>
                        <h2>Blocking the robots: robots.txt</h2>
                        <ul>
                            <li><a href="http://www.robotstxt.org">Standard format</a> for telling robots what not to crawl.</li>
                            <li>Always found at root of website (http://example.ca/robots.txt).</li>
                            <li>Specifies web site directories to block; default is to allow everything.</li>
                            <li>Obeyed by well-behaved robots (includes major search engines).</li>
                        </ul>
                        <aside class="notes">
                            <p>
                                "robots" are the affectionate name for the scripts/processes/machines that
                                crawl through websites to extract content and process it for some purpose.
                                Search engines like Google and Bing use robots to grab content from sites so
                                that they can serve up links to your pages/images/library resources in search
                                results; other robots might be interested only in product information so that
                                they can tell you the cheapest place to buy something online.
                            </p>
                        </aside>
                    </section>

                    <section>
                        <h2>Do block:</h2>
                        <p>Search results, facets, filters, etc</p>
                        <pre><code data-trim>
http://catalogue.example.com/search?term=fish 
http://catalogue.example.com/search?term=fishes
http://catalogue.example.com/search?term=fishing
                        </code></pre>
                        <p>Search engines will crawl variants of these forever.</p>
                        <p class="fragment">Block them!</p>
                        <pre class="fragment"><code data-trim>
User-agent: *
Disallow: /search/
                        </code></pre>
                        <aside class="notes">
                            <p>
                                Robots are pretty dumb and easily confused. For example, they'll find
                                a search results page on your catalogue that lists thousands of results for the
                                term "fish" and dutifully crawl through all of the links that they find; then
                                if they find a search results page for the term "fishing" that (due to
                                stemming) returns exactly the same results, they'll dutifully crawl through all
                                of the links they find again.
                            </p>
                            <pre>
                            http://catalogue.example.com/results?term=fish 
                            http://catalogue.example.com/results?term=fishing 
                            </pre>
                            <p>
                                often lead to results like:
                            </p>
                            <pre>
                            http://catalogue.example.com/record/1?term=fish 
                            http://catalogue.example.com/record/1?term=fishing
                            </pre>
                            <p>
                                Even though you and I have no problem guessing that the page will display the
                                same record, and that (if anything) the list of related search results might
                                change on that page without affecting the substantive content on the page, a
                                search engine has no way of being able to telling what it will find on the
                                other end of these links without human intervention.
                            </p>
                        </aside>
                    </section>

                    <section>
                        <h2>Don't block everything!</h2>
                        <pre><code data-trim>
User-agent: *
Disallow: /
                        </code></pre>
                        <p>This is not a good thing to see on your site.</p>
                        <aside class="notes">
                            <p>
                                That effectively tells every robot to just go away. If you don't want your
                                content to show up in search engines, that's the right thing to do--but most of
                                us are here because we wantsearch engines to expose users to the library
                                resources that we have. Note that search engines have become much smarter over
                                the past 20 years, and will dynamically adapt their crawl rate to avoid
                                bringing down your web resources… because, after all, they want to connect
                                users to the best resources on the web, and a web site that isn't running isn't
                                very much use to them.
                                </p>
                                <p>
                                One other note: robots.txt only affects well-behaved robots. Again, search
                                engines like Google, Bing, and Yandex are interested in playing by the rules,
                                so if you find that your site is constantly being brought down by too much
                                traffic, it's unlikely that it's the well-behaved search engines causing the
                                problem. Setting a highly restrictive robots.txt file is more likely to just
                                block the good robots while doing nothing to prevent the bad robots from
                                continuing to pound your site.
                                </p>
                        </aside>
                    </section>
                </section>

                <section>
                    <section>
                        <h2>Guiding the machines: sitemaps</h2>
                        <ul>
                            <li><a href="http://sitemaps.org">XML format</a> that lists every URL of interest for robots</li>
                        </ul>
                        <pre><code data-trim class="xml">
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
   <url>
      <loc>http://example.com/about</loc>
      <lastmod>2015-01-29</lastmod>
   </url>
   <url>
      <loc>http://example.com/hours</loc>
      <lastmod>2015-01-25</lastmod>
   </url>
</urlset>
                        </code></pre>
                        <aside class="notes">
                            If you're thinking "Wait, my catalogue has over a million items in it; and my
                            web site has new pages added to it every couple of days; that sounds like a lot
                            of work!", you're right--it's a lot of work for humans. But it's something that
                            a machine can easily generate on a nightly basis. And many of the content
                            management systems that websites run on either have the ability built-in, or
                            have plugins available, that will take care of generating sitemaps for you.
                            Similarly, library systems like Evergreen and VuFind offer the ability to
                            generate sitemaps for you; you just need to set it up once and forget about it.
                        </aside>
                    </section>

                    <section>
                        <h2>Advertise your sitemap</h2>
                        <p>Just add a Sitemap: line to your <code>robots.txt</code> file:</p>
                        <pre><code data-trim>
Sitemap: https://example.ca/sitemapindex.xml
                        </code></pre>
                        <aside class="notes">
                            <p>
                            Most search engines will find and use that the next time they refresh their
                            knowledge of your robots.txt file.
                            </p>
                            <p>
                            However, you can also submit the location of your sitemap directly to Google,
                            Bing, and Yandex; each offers a "Webmaster tools" site that you can use to
                            track the status of the indexing of your sites, some insight into the usage of
                            your site and potential errors found at your site, and potentially influence
                            how the search engine interprets your content. Diving into those tools could be
                            the subject of a whole different talk, though!
                            </p>
                        </aside>
                    </section>
                </section>

                <section>
                    <h2>Structured data: why?</h2>
                    <p>
                        Even clean HTML is still just a bag of words and media to machines.
                    </p>
                    <aside class="notes">
                        <p>
                        However, there is only so much that machines can glean from well-structured
                        HTML. It's still a bag of words to them, albeit with suggestions about what the
                        titles of various sections might be, but it can be very challenging for them to
                        determine (for example) what the right picture might be on a news page to
                        associate with the event. Google News provides lots of amusing examples of
                        machines guessing wrong.
                        </p>
                    </aside>
                </section>

            </div>

        </div>

        <script src="lib/js/head.min.js"></script>
        <script src="js/reveal.js"></script>

        <script>

            // Full list of configuration options available at:
            // https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                controls: true,
                progress: true,
                history: true,
                center: true,

                transition: 'slide', // none/fade/slide/convex/concave/zoom

                // Optional reveal.js plugins
                dependencies: [
                    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/zoom-js/zoom.js', async: true },
                    { src: 'plugin/notes/notes.js', async: true }
                ]
            });

        </script>

    </body>
</html>
